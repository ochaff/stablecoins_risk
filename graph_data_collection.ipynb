{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f766d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import requests\n",
    "import pprint\n",
    "import time\n",
    "import textwrap\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "sns.set_theme('notebook', 'white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2c77e4",
   "metadata": {},
   "source": [
    "Include .env file with an api key under GRAPH_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c37836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.environ.get(\"GRAPH_API_KEY\") \n",
    "def run_query(query: str, variables: dict | None = None):\n",
    "    payload = {\"query\": query, \"variables\": variables or {}}\n",
    "    r = requests.post(ENDPOINT, json=payload)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    if \"errors\" in data:\n",
    "        raise RuntimeError(data[\"errors\"])\n",
    "    return data[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444d539",
   "metadata": {},
   "source": [
    "Choose a subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed3d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBGRAPH_ID = \"JCNWRypm7FYwV8fx5HhzZPSFaMxgkPuw4TnR3Gpi81zk\" \n",
    "ENDPOINT = f\"https://gateway.thegraph.com/api/{API_KEY}/subgraphs/id/{SUBGRAPH_ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2958be",
   "metadata": {},
   "source": [
    "Print its table names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22399908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['token',\n",
      " 'tokens',\n",
      " 'rewardToken',\n",
      " 'rewardTokens',\n",
      " 'interestRate',\n",
      " 'interestRates',\n",
      " 'fee',\n",
      " 'fees',\n",
      " 'revenueDetail',\n",
      " 'revenueDetails',\n",
      " 'oracle',\n",
      " 'oracles',\n",
      " 'lendingProtocol',\n",
      " 'lendingProtocols',\n",
      " 'marketList',\n",
      " 'marketLists',\n",
      " 'usageMetricsDailySnapshot',\n",
      " 'usageMetricsDailySnapshots',\n",
      " 'usageMetricsHourlySnapshot',\n",
      " 'usageMetricsHourlySnapshots',\n",
      " 'financialsDailySnapshot',\n",
      " 'financialsDailySnapshots',\n",
      " 'market',\n",
      " 'markets',\n",
      " 'marketDailySnapshot',\n",
      " 'marketDailySnapshots',\n",
      " 'marketHourlySnapshot',\n",
      " 'marketHourlySnapshots',\n",
      " 'account',\n",
      " 'accounts',\n",
      " 'position',\n",
      " 'positions',\n",
      " 'positionSnapshot',\n",
      " 'positionSnapshots',\n",
      " 'activeAccount',\n",
      " 'activeAccounts',\n",
      " 'txSigner',\n",
      " 'txSigners',\n",
      " 'positionCounter',\n",
      " 'positionCounters',\n",
      " 'deposit',\n",
      " 'deposits',\n",
      " 'withdraw',\n",
      " 'withdraws',\n",
      " 'borrow',\n",
      " 'borrows',\n",
      " 'repay',\n",
      " 'repays',\n",
      " 'liquidate',\n",
      " 'liquidates',\n",
      " 'transfer',\n",
      " 'transfers',\n",
      " 'flashloan',\n",
      " 'flashloans',\n",
      " 'defaultOracle',\n",
      " 'defaultOracles',\n",
      " 'flashLoanPremium',\n",
      " 'flashLoanPremiums',\n",
      " 'protocol',\n",
      " 'protocols',\n",
      " 'event',\n",
      " 'events',\n",
      " '_meta']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "introspection = \"\"\"\n",
    "{\n",
    "__schema {\n",
    "queryType {\n",
    "fields { name }\n",
    "}\n",
    "}\n",
    "}\n",
    "\"\"\"\n",
    "root_fields = run_query(introspection)[\"__schema\"][\"queryType\"][\"fields\"]\n",
    "pprint.pprint([f[\"name\"] for f in root_fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f4c6fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gql(q: str, v: dict | None = None):\n",
    "    r = requests.post(ENDPOINT, json={\"query\": q, \"variables\": v or {}})\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    if \"errors\" in data:\n",
    "        raise RuntimeError(json.dumps(data[\"errors\"], indent=2))\n",
    "    return data[\"data\"]\n",
    "\n",
    "def show_fields(type_name: str):\n",
    "    q = \"\"\"\n",
    "    query($t: String!){\n",
    "      __type(name: $t){\n",
    "        name\n",
    "        fields { name }\n",
    "      }\n",
    "    }\"\"\"\n",
    "    fields = gql(textwrap.dedent(q), {\"t\": type_name})[\"__type\"][\"fields\"]\n",
    "    print(f\"\\nFields on {type_name}:\")\n",
    "    print(\", \".join(f[\"name\"] for f in fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c88aac59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fields on Liquidate:\n",
      "id, hash, nonce, logIndex, gasPrice, gasUsed, gasLimit, blockNumber, timestamp, liquidator, liquidatee, market, positions, asset, amount, amountUSD, profitUSD\n"
     ]
    }
   ],
   "source": [
    "show_fields(\"Liquidate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79635f12",
   "metadata": {},
   "source": [
    "# AAVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5dd21",
   "metadata": {},
   "source": [
    "## Hourly snapshots of relevant pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6715ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "USDC_ADDRESS = \"0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48\"  \n",
    "USDT_ADDRESS = \"0xdac17f958d2ee523a2206206994597c13d831ec7\"  \n",
    "DAI_ADDRESS = \"0x6B175474E89094C44Da98b954EedeAC495271d0F\"\n",
    "WETH_ADDRESS = '0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2'\n",
    "USDE_ADDRESS = \"0x4c9EDD5852cd905f086C759E8383e09bff1E68B3\"\n",
    "\n",
    "def gql(query: str, variables: dict | None = None) -> dict:\n",
    "    while True:\n",
    "        r = requests.post(ENDPOINT, json={\"query\": query, \"variables\": variables or {}})\n",
    "        if r.status_code == 429:                           # hit the rate limit → wait & retry\n",
    "            time.sleep(int(r.headers.get(\"Retry-After\", \"2\")))\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        if \"errors\" in data:\n",
    "            raise RuntimeError(json.dumps(data[\"errors\"], indent=2))\n",
    "        return data[\"data\"]\n",
    "\n",
    "def get_data_coin_aave(name):\n",
    "  coins = {\n",
    "      \"usdt\": USDT_ADDRESS,\n",
    "      \"usdc\": USDC_ADDRESS,\n",
    "      \"dai\": DAI_ADDRESS,\n",
    "      \"eth\": WETH_ADDRESS,\n",
    "      \"usde\" : USDE_ADDRESS\n",
    "  }\n",
    "\n",
    "  coin_address = coins[name]\n",
    "\n",
    "  MKT_QUERY = \"\"\"\n",
    "  query($token: String!) {\n",
    "    markets(where: {inputToken: $token}) {\n",
    "      id\n",
    "      name\n",
    "      inputToken { symbol }\n",
    "    }\n",
    "  }\n",
    "  \"\"\"\n",
    "  markets = gql(MKT_QUERY, {\"token\": coin_address})[\"markets\"]\n",
    "  if not markets:\n",
    "      raise ValueError(\"USDT market not found in this subgraph.\")\n",
    "  MARKET_ID = markets[0][\"id\"]\n",
    "  print(\"Found market:\", markets[0][\"name\"], \"→\", MARKET_ID)\n",
    "\n",
    "\n",
    "  SNAP_QUERY = \"\"\"\n",
    "  query($mkt: String!, $first: Int!, $skip: Int!) {\n",
    "    marketHourlySnapshots(\n",
    "      where: {market: $mkt}\n",
    "      orderBy: timestamp\n",
    "      orderDirection: asc\n",
    "      first: $first\n",
    "      skip: $skip\n",
    "    ) {\n",
    "      id\n",
    "      timestamp\n",
    "      blockNumber\n",
    "      totalValueLockedUSD\n",
    "      totalDepositBalanceUSD\n",
    "      totalBorrowBalanceUSD\n",
    "      hourlyLiquidateUSD\n",
    "      inputTokenBalance\n",
    "      rates {\n",
    "        side          # LENDER / BORROWER\n",
    "        type          # VARIABLE / STABLE\n",
    "        rate          # per-second ray (1e27)\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \"\"\"\n",
    "\n",
    "  def fetch_daily_snapshots(mkt_id: str, batch: int = 1000) -> list[dict]:\n",
    "      out, skip = [], 0\n",
    "      while True:\n",
    "          chunk = gql(SNAP_QUERY, {\"mkt\": mkt_id, \"first\": batch, \"skip\": skip})\\\n",
    "                      [\"marketHourlySnapshots\"]\n",
    "          if not chunk:\n",
    "              break\n",
    "          out.extend(chunk)\n",
    "          skip += len(chunk)\n",
    "      return out\n",
    "\n",
    "  raw_snaps = fetch_daily_snapshots(MARKET_ID)\n",
    "  print(\"Fetched\", len(raw_snaps), \"hourly snapshots\")\n",
    "\n",
    "  def to_row(snap: dict) -> dict:\n",
    "      row = {\n",
    "          \"timestamp\"      : int(snap[\"timestamp\"]),\n",
    "          \"block\"          : int(snap[\"blockNumber\"]),\n",
    "          \"TVL_USD\"        : float(snap[\"totalValueLockedUSD\"]),\n",
    "          \"supplied_USD\"   : float(snap[\"totalDepositBalanceUSD\"]),\n",
    "          \"borrowed_USD\"   : float(snap[\"totalBorrowBalanceUSD\"]),\n",
    "          \"liquidations\"   : float(snap['hourlyLiquidateUSD']),\n",
    "          \"token_balance\"  : float(snap[\"inputTokenBalance\"]),\n",
    "      }\n",
    "\n",
    "      # initialise APR columns with NaN\n",
    "      for side in (\"lender\", \"borrower\"):\n",
    "          for typ in (\"variable\", \"stable\"):\n",
    "              row[f\"{side}_{typ}_apr\"] = None\n",
    "\n",
    "      SECONDS_PER_YEAR = 60 * 60 * 24 * 365\n",
    "      for r in snap[\"rates\"]:\n",
    "          col = f\"{r['side'].lower()}_{r['type'].lower()}_apr\"\n",
    "          row[col] = float(r[\"rate\"])\n",
    "          # row[col] = float(r[\"rate\"]) / 1e27 * SECONDS_PER_YEAR * 100  # % APR\n",
    "      return row\n",
    "\n",
    "  df = pd.DataFrame(map(to_row, raw_snaps))\n",
    "  df[\"date\"] = pd.to_datetime(df[\"timestamp\"], unit=\"s\")\n",
    "  df.set_index(\"date\", inplace=True)\n",
    "  df.sort_index(inplace=True)\n",
    "\n",
    "  df.index = df.index.ceil('h')\n",
    "  df = df.groupby(level = 0).mean()\n",
    "  full_idx = pd.date_range(df.index[0].ceil(\"H\"),\n",
    "                          df.index[-1].ceil(\"H\"),\n",
    "                          freq=\"1H\")\n",
    "  df = df.reindex(full_idx,method = 'ffill')\n",
    "  df.to_parquet(f'aave_v3_{name}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80476cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found market: Aave Ethereum USDT → 0x23878914efe38d27c4d67ab83ed1b93a74d4086a\n",
      "Fetched 23212 daily snapshots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207226/4181420643.py:116: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_idx = pd.date_range(df.index[0].ceil(\"H\"),\n",
      "/tmp/ipykernel_207226/4181420643.py:117: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df.index[-1].ceil(\"H\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found market: Aave Ethereum USDC → 0x98c23e9d8f34fefb1b7bd6a91b7ff122f4e16f5c\n",
      "Fetched 24665 daily snapshots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207226/4181420643.py:116: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_idx = pd.date_range(df.index[0].ceil(\"H\"),\n",
      "/tmp/ipykernel_207226/4181420643.py:117: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df.index[-1].ceil(\"H\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found market: Aave Ethereum DAI → 0x018008bfb33d285247a21d44e50697654f754e63\n",
      "Fetched 19193 daily snapshots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207226/4181420643.py:116: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_idx = pd.date_range(df.index[0].ceil(\"H\"),\n",
      "/tmp/ipykernel_207226/4181420643.py:117: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df.index[-1].ceil(\"H\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found market: Aave Ethereum WETH → 0x4d5f47fa6a74757f35c14fd3a6ef8e3c9bc514e8\n",
      "Fetched 24937 daily snapshots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207226/4181420643.py:116: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_idx = pd.date_range(df.index[0].ceil(\"H\"),\n",
      "/tmp/ipykernel_207226/4181420643.py:117: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df.index[-1].ceil(\"H\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found market: Aave Ethereum USDe → 0x4f5923fc5fd4a93352581b38b7cd26943012decf\n",
      "Fetched 8380 daily snapshots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207226/4181420643.py:116: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_idx = pd.date_range(df.index[0].ceil(\"H\"),\n",
      "/tmp/ipykernel_207226/4181420643.py:117: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df.index[-1].ceil(\"H\"),\n"
     ]
    }
   ],
   "source": [
    "for coin in ['usdt','usdc','dai','eth','usde']:\n",
    "    get_data_coin_aave(coin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5ed46a",
   "metadata": {},
   "source": [
    "## Hourly $ amount of liquidations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b59e71a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           liquidation_usd\n",
      "datetime                                  \n",
      "2023-01-30 19:00:00+00:00      1322.616413\n",
      "2023-01-30 20:00:00+00:00         0.000000\n",
      "2023-01-30 21:00:00+00:00         0.000000\n",
      "2023-01-30 22:00:00+00:00         0.000000\n",
      "2023-01-30 23:00:00+00:00         0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207226/4019883245.py:93: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  .resample(\"1H\")\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, requests\n",
    "import pandas as pd\n",
    "\n",
    "API_KEY      = os.getenv(\"GRAPH_API_KEY\") or \"YOUR_API_KEY_HERE\"\n",
    "SUBGRAPH_ID  = \"JCNWRypm7FYwV8fx5HhzZPSFaMxgkPuw4TnR3Gpi81zk\"\n",
    "ENDPOINT     = f\"https://gateway.thegraph.com/api/{API_KEY}/subgraphs/id/{SUBGRAPH_ID}\"\n",
    "\n",
    "# ---------- GraphQL helper ---------------------------------------------------\n",
    "def gql(query: str, variables: dict | None = None) -> dict:\n",
    "    while True:\n",
    "        r = requests.post(ENDPOINT, json={\"query\": query, \"variables\": variables or {}})\n",
    "        if r.status_code == 429:                # rate-limit → wait & retry\n",
    "            time.sleep(int(r.headers.get(\"Retry-After\", \"2\")))\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        if \"errors\" in data:\n",
    "            raise RuntimeError(json.dumps(data[\"errors\"], indent=2))\n",
    "        return data[\"data\"]\n",
    "\n",
    "# ---------- liquidation fetcher (all collaterals) ---------------------------\n",
    "LIQ_QUERY_ALL = \"\"\"\n",
    "query($first:Int!, $skip:Int!){\n",
    "  liquidates(\n",
    "    orderBy: timestamp\n",
    "    orderDirection: asc\n",
    "    first: $first\n",
    "    skip:  $skip\n",
    "  ){\n",
    "    id\n",
    "    hash\n",
    "    nonce\n",
    "    logIndex\n",
    "    gasPrice\n",
    "    gasUsed\n",
    "    gasLimit\n",
    "    blockNumber\n",
    "    timestamp\n",
    "    liquidator\n",
    "    liquidatee\n",
    "    market { id }\n",
    "    positions { id }\n",
    "    asset\n",
    "    amount\n",
    "    amountUSD\n",
    "    profitUSD\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def fetch_all_liquidations(batch: int = 1000) -> pd.DataFrame:\n",
    "    \"\"\"Fetch all liquidations (all collateral assets) as a raw events DataFrame.\"\"\"\n",
    "    rows, skip = [], 0\n",
    "    while True:\n",
    "        data = gql(LIQ_QUERY_ALL, {\"first\": batch, \"skip\": skip})[\"liquidates\"]\n",
    "        if not data:\n",
    "            break\n",
    "        rows.extend(data)\n",
    "        skip += len(data)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # normalise nested objects for pandas\n",
    "    for r in rows:\n",
    "        r[\"market\"]    = r[\"market\"][\"id\"] if r[\"market\"] else None\n",
    "        r[\"positions\"] = \",\".join(p[\"id\"] for p in r[\"positions\"]) if r[\"positions\"] else \"\"\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # timestamp → datetime index\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"timestamp\"].astype(int), unit=\"s\", utc=True)\n",
    "    df.set_index(\"datetime\", inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # convert amountUSD to numeric\n",
    "    df[\"amountUSD\"] = pd.to_numeric(df[\"amountUSD\"], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def fetch_hourly_liquidations_usd(batch: int = 1000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch all liquidations and return an hourly time series\n",
    "    of total liquidation amount in USD (across all collaterals).\n",
    "    \"\"\"\n",
    "    df = fetch_all_liquidations(batch=batch)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # Aggregate to hourly bins\n",
    "    hourly = (\n",
    "        df[\"amountUSD\"]\n",
    "        .resample(\"1H\")\n",
    "        .sum()\n",
    "        .fillna(0.0)\n",
    "        .to_frame(name=\"liquidation_usd\")\n",
    "    )\n",
    "    return hourly\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hourly_liqs = fetch_hourly_liquidations_usd(batch=1000)\n",
    "    print(hourly_liqs.head())\n",
    "\n",
    "    # Save to parquet if desired\n",
    "    hourly_liqs.to_parquet(\"aave_hourly_liquidations.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acabff2",
   "metadata": {},
   "source": [
    "# Uniswap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9be6c",
   "metadata": {},
   "source": [
    "## Relevant Pool Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c902d5",
   "metadata": {},
   "source": [
    "### stable-stable pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "298a9de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.environ.get(\"GRAPH_API_KEY\") \n",
    "SUBGRAPH_ID = \"5zvR82QoaXYFyDEKLZ9t6v9adgnptxYpKpSbxtgVENFV\" \n",
    "ENDPOINT = f\"https://gateway.thegraph.com/api/{API_KEY}/subgraphs/id/{SUBGRAPH_ID}\"\n",
    "\n",
    "def run_query(query: str, variables: dict | None = None):\n",
    "    payload = {\"query\": query, \"variables\": variables or {}}\n",
    "    r = requests.post(ENDPOINT, json=payload)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    if \"errors\" in data:\n",
    "        raise RuntimeError(data[\"errors\"])\n",
    "    return data[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17baaedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stable_pool(token0,token1):\n",
    "    stables = {\n",
    "        \"USDC\": \"0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48\",\n",
    "        \"USDT\": \"0xdac17f958d2ee523a2206206994597c13d831ec7\",\n",
    "        \"DAI\":  \"0x6b175474e89094c44da98b954eedeac495271d0f\",\n",
    "        \"USDE\": \"0x4c9edd5852cd905f086c759e8383e09bff1e68b3\",\n",
    "    }\n",
    "    stables_r = {y:x for (x,y) in stables.items()}\n",
    "    pools_query = \"\"\"\n",
    "    query ($token0: String!, $token1: String!) {\n",
    "    pools(\n",
    "        where: {\n",
    "        token0_in: [$token0, $token1]\n",
    "        token1_in: [$token0, $token1]\n",
    "        }\n",
    "    ) {\n",
    "        id\n",
    "        feeTier\n",
    "        totalValueLockedUSD\n",
    "        token0 { id symbol }\n",
    "        token1 { id symbol }\n",
    "    }\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    pools = run_query(pools_query, {\"token0\": stables[token0], \"token1\": stables[token1]})[\"pools\"]\n",
    "    POOL_ID = pools[0]['id']\n",
    "    t0 = stables_r[f'{pools[0][\"token0\"][\"id\"]}']\n",
    "    t1 = stables_r[f'{pools[0][\"token1\"][\"id\"]}'] \n",
    "    pool_hour_query = \"\"\"\n",
    "    query ($poolId: String!, $skip: Int!) {\n",
    "    poolHourDatas(\n",
    "        first: 1000\n",
    "        skip: $skip\n",
    "        orderBy: periodStartUnix\n",
    "        orderDirection: asc\n",
    "        where: { pool: $poolId }\n",
    "    ) {\n",
    "        periodStartUnix\n",
    "        token0Price\n",
    "        token1Price\n",
    "        volumeToken0\n",
    "        volumeToken1\n",
    "        volumeUSD\n",
    "        tvlUSD\n",
    "    }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    all_rows = []\n",
    "    skip = 0\n",
    "    while True:\n",
    "        batch = run_query(pool_hour_query, {\"poolId\": POOL_ID, \"skip\": skip})[\"poolHourDatas\"]\n",
    "        if not batch:\n",
    "            break\n",
    "        all_rows.extend(batch)\n",
    "        skip += len(batch)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"periodStartUnix\"].astype(int), unit=\"s\")\n",
    "    df[\"token0Price\"] = df[\"token0Price\"].astype(float)\n",
    "    df[\"token1Price\"] = df[\"token1Price\"].astype(float)\n",
    "    df[\"volumeUSD\"] = df[\"volumeUSD\"].astype(float)\n",
    "    df[\"volumeToken1\"] = df[\"volumeToken1\"].astype(float)\n",
    "    df[\"volumeToken0\"] = df[\"volumeToken0\"].astype(float)\n",
    "    df[\"tvlUSD\"] = df[\"tvlUSD\"].astype(float)\n",
    "\n",
    "    # keep nice columns\n",
    "    df = df[[\"timestamp\", \"token0Price\", \"token1Price\", \"volumeUSD\", \"tvlUSD\"]]\n",
    "    df = df.iloc[100:]\n",
    "    df = df.rename(columns = {'token1Price' : t1,'token0Price' : t0})\n",
    "    df.index = df['timestamp']\n",
    "    df = df.drop(columns = ['timestamp'])\n",
    "    df = df.resample('h').mean()\n",
    "    df.to_parquet(f'{t0}_{t1}_pool')\n",
    "\n",
    "    pool_day_query = \"\"\"\n",
    "    query ($poolId: String!, $skip: Int!) {\n",
    "    poolDayDatas(\n",
    "        first: 1000\n",
    "        skip: $skip\n",
    "        orderBy: date\n",
    "        orderDirection: asc\n",
    "        where: { pool: $poolId }\n",
    "    ) {\n",
    "        date\n",
    "        token0Price\n",
    "        token1Price\n",
    "        volumeToken0\n",
    "        volumeToken1\n",
    "        volumeUSD\n",
    "        tvlUSD\n",
    "    }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    all_rows = []\n",
    "    skip = 0\n",
    "    while True:\n",
    "        batch = run_query(pool_day_query, {\"poolId\": POOL_ID, \"skip\": skip})[\"poolDayDatas\"]\n",
    "        if not batch:\n",
    "            break\n",
    "        all_rows.extend(batch)\n",
    "        skip += len(batch)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"date\"].astype(int), unit=\"s\")\n",
    "    df[\"volumeUSD\"] = df[\"volumeUSD\"].astype(float)\n",
    "    df[\"tvlUSD\"] = df[\"tvlUSD\"].astype(float)\n",
    "    df = df[[\"timestamp\",\"volumeUSD\", \"tvlUSD\"]]\n",
    "    df = df.iloc[1:]\n",
    "    df.index = df['timestamp']\n",
    "    df = df.drop(columns = ['timestamp'])\n",
    "    df = df.resample('D').mean()\n",
    "    df['vol_rate'] = df['volumeUSD']/df['tvlUSD']\n",
    "    df.to_parquet(f'{t0}_{t1}_pool_volume_daily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3b0dce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in [['USDT','USDC'],['USDC','DAI'],['USDE','USDT']]:\n",
    "    collect_stable_pool(pair[0],pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84819497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
